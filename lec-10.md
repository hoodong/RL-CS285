# Lecture 10. Optimal Control and Planning

## TOC
- model-based RL
- open-loop planning
- stochastic optimization methods
- monte carlo tree search (MCTS)
- trajectory optimization

## 질문
- model-free RL vs model-based RL
  - model-free RL은 real experiance만 사용
  - model-based RL은 real experiance 뿐만 아니라 simulated experiance도 사용
  - real experiance는 환경과 상호작용을 통해, simulated experiance는 model로부터 얻어짐
  - Why Choose Model-Based Reinforcement Learning? (Brian Douglas)
    - https://www.youtube.com/watch?v=ztT2ZLWTfXw&t=1s
- 용어
  - planning?
    - (Sutton 8.1) any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment
  - control과 planning의 차이?
  - prediction과 control의 차이?
  - open-loop와 closes-loop의 차이?
- 환경의 transition dynamic을 아는 경우
  - deterministic case?  
  - stochastic open-loop case?
  - stochastic closed-loop case?
- stochastic 인 경우에 환경/정책 중에 어떤 것이 random한가? 환경
- deterministic인 경우는 왜 open-loop와 closed-loop로 나누지 않는지? 
- open-loop planning (이 분류가 맞는지?)
  - stochastic optimization
    - random shooting
    - cross-entropy method (CEM)
    - Monte Carlo tree search (MCTS)
  - trajectory optimization
    - linear quadratic regulator (LQR)
- p15: CEM (cross-entropy method)
  - (Wiki) a Monte Carlo method for importance sampling and optimization.
  - (Krose 2013) The CE method can be viewed as an adaptive importance sampling procedure that uses the cross-entropy or Kullback–Leibler divergence as a measure of closeness between two sampling distributions. The CE method is particularly useful for the estimation of rare-event probabilities.
  - 알고리즘 (Wiki)
    - step 1: obtain N samples from current sampling distribution
    - step 2: evaluate objective function at sampled points
    - step 3: sort X by objective function values in descending order
    - step 4: update parameters of sampling distribution via elite samples
    - repeat step 1 to step 4 until stopping criterion
  - 왜 cross-entropy 방법이라고 부를까? 이 방법이CE를 최소화 할까? 
  - importance sampling이 어디서 사용되는 걸까?
- p20: MCTS (MC tree search)
  - 현재 상태에서 planning을 통해(모델을 이용해) 행동을 결정(트리를 탐색)하는 문제
    - 현재 상태(root node)에서 행동가치를 추정하기 위함
    - 모든 tree를 탐색할 수 없으니 어디부터 탐색할지 정해야 한다.
    - 보상이 크고 방문 횟수가 적은 노드부터 탐색하자. 
  - 알고리즘
    - step 1: root node에서 TreePolicy를 이용해 leaf node를 찾는다.(selection, expansion)
    - step 2: leaf node에서 DefaultPolicy를 이용해 leaf node의 가치를 평가한다.(rollout)
    - step 3: root node와 leaf node 사이의 모든 가치를 업데이트 한다.(backup)
    - step 1~3을 반복한다.
    - root node에서 최선의 행동의 취한다.
  - step 2가 왜 필요한지? (rollout을 이용해 leaf node의 가치 계산)
  - TreePolicy는 UCT(upper confidence bounding tree)를 사용한다
    - UCT는 confidence interval의 upper bound를 계산한다. (by Hoeffding's inequality)
  - 일반적인 MC control과 MCTS의 차이점이 무엇인지?
    - root node에서 rollout 하지 않고 leaf node에서rollout하는 이유는?
- p23: trajectory optimization with derivative
  - 앞에서는 미분을 사용하지 않았지만(black optimization), 이제 미분을 안다면?
  - optimal control에서는 상태를 $x_t$, 행동을 $u_t$로 표기한다.
- p25: shooting method와 collocation method의 차이?
- p26: LQR (linear quadratic regulator)  
  - 최적제어: 물리적인 제약조건을 만족하면서 성능지표 또는 목적함수를 최적화하도록 동적 시스템(dynamic system)의 제어변수을 결정하는 문제
  - linear-quadratic system: 시스템이 linear, 목적함수가 quadratic인 시스템  
  - (chatgpt3.5) LQR은 시스템의 상태를 조절하여 원하는 목표를 달성하는 제어기를 설계하는 방법 중 하나로서, 시스템 모델링/성능 지표 정의/최적제어 입력 계산/피드백 제어기 설계로 구성된다.
- p27: linear인 경우에 LQR
  - cost $c(x_t,u_t)$을 2차로 근사할 때 $C_t$는 Hessian, $c_t$는 gradient를 의미할까?
  - dynamics $f(x_t,u_t)$을 1차로 근사할 때 $F_t$와 $f_t$의 물리적 의미는 무엇일까?
  - 비용함수에서 $C_t$는 Hessian, $c_t$는 gradient를 의미할까? (테일러 2차 전개)
  - 마지막 행동 $u_T$는 목적함수의 마지막 항 $c(x_t,u_t)$에만 영향을 미친다.  
    과거 비용 $c(x_1,u_1),...,c(x_{T-1,u_{T-1}}$에는 영향을 주지 않는다.
  - 따라서 마지막 cost-to-go는 $Q(x_T,u_T)=c(x_T,u_T)$
  - $Q(x_T,u_T)$는 2차식이므로 $u_T$에 대해 미분하면 최적행동 $u_T$를 얻을 수 있다.
- p27-30: LQR 알고리즘 유도에 필요한 관계식
  - 최적 행동과 현재 상태의 관계: $u_t = K_t x_t + k_t$
  - 현재 V = 현재 Q에서 최적행동을 취한 것
    $V(x_t)=Q(x_t,K_t x_t + k_t)$ ($u_t$에 최적 행동을 대입)
  - 과거 Q = 현재 비용 + 현재 V
    $Q(x_{t-1},u_{t-1})=c(x_{t-1},u_{t-1})+V(x_{t})$
- p31: LQR 알고리즘에서 계산하는 것
  - given
    - 모델 파라미터(linear): $F,f$ 
    - 목적함수 파라미터(quadratic): $C,c$
    - 시작 상태: $x_1$
  - backward recursion: $t=T$ to $1$
    - $Q_t = C+F^T V_{t+1} F$
    - $q_t = c+F_t^T V_{t+1} f_t + F_t^T v_{t+1}$
    - $K_t = -Q_{u_t u_t}^{-1} Q_{u_t x_t}$
    - $k_t = -Q_{u_t u_t}^{-1} q_{u_t}$
    - $V_t = Q_{x_t x_t}+Q_{x_t u_t}K_t+K_t^T Q_{u_t x_t}+K_t^T Q_{u_t u_t}K_t$
    - $v_t = q_{x_t}+Q_{x_t u_t}k_t +K_t^T q_{u_t}+K_t^T Q_{u_t u_t}k_t$
  - forward recursion: $t=1$ to $T$
    - $u_t = K_t x_t + k_t$
    - $x_{t+1} = f(x_t,u_t)$
  - backward recursion에서 모든 $K_t,k_t$를 저장해 둬야 하나? forward recursion에서 쓰기 위해서?
- p34: stochastic인 경우에 LQR
- p36: nonlinear인 경우에 LQR
  - nonlinear system을 linear-quadratic system으로 근사한다.
  - iterative LQR (iLQR)
  - p39: iLQR는 Newton's method의 근사
  - p40: differential dynamic programming (DDP)는 시스템을 2차로 근사
    
