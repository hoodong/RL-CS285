# Lecture 12. Model-based Policy Learning

## keywords
- model-free learning with a model
- Dyna-style algorithm
- multi-step models & successor representations

## 질문 및 요약
- p2: 지난 강의에서는 model-based RL ver 1.5을 다루었다.
  - 모델 오류를 보정하기 위해 매 시간 스텝마다 replanning을 수행한다.(MPC)
  - 하지만 open-loop planning을 쓰면 큰 단점이 발생한다.
- p3: open-loop contol은 최적이 아니다.
  - 예를 들어 간단한 한자리 덧셈 시험을 생각해 보자.
  - 두 가지 연속된 행동을 정해야 한다. 1.시험 참가여부, 2.답안 제출(덧셈 계산)
  - 보상은 답을 맞추면 2000달러, 답을 틀리면 -1000달러, 시험을 안보면 0이다.
  - open-loop contol에서는 시험 문제를 보지도 않고 답을 정해야 한다.
  - 따라서 최적 plan은 시험에 참가하지 않는 것이다. (문제를 보지 않고 답을 맞출 확률이 매우 낮기 때문에)
  - MPC를 쓰더라도 이 문제는 해결할 수 없다. (시험에 참가하지 않을 것이므로)
  - 이 문제를 해결하려면 closed-loop control을 써야 한다.
- p4: closed-loop control
  - 행동이 아니라 정책을 생성한다.
  - 앞의 예의 경우에 모든 한자리 덧셈 문제에 대한 답을 계산해 둔다.
  - 따라서 최적 plan은 덧셈 시험에 참가하고 답을 계산하는 것이다.
  - 그러면 어떤 형태의 정책을 써야 할까? global vs local
  - LQR은 open-loop plan 근처에서만 유효한 local 정책이다.
  - 신경망은 모든 상태에 대한 행동을 생성하는 global 정책이다.
- p5: model-based RL v2.0  
  - 학습된 모델이 있을 때 총 보상을 최대화하도록 정책을 학습시켜야 한다.
  - 계산 그래프는 3개의 함수로 구성된다: policy $\pi$, dynamics $f$, reward $r$
  - 보상함수는 안다고 가정, 정책과 다이나믹은 신경망, 손실함수는 누적보상의 음수이다.
  - 각 보상 노드에서 역전파를 수행해 정책 파라미터에 대한 누적보상의 그래디언트를 계산한다.
  - ver 1.5와 비교해보면 planning이 정책 학습으로 바뀌었다.
  - model-free policy gradient와 달리 다이나믹의 모형이 gradient 계산에 포함된다.(model-based policy gradient)
- p8: 역전파를 이용해 정책파라미터를 최적화할 때 문제점
  - 먼저 수행된 행동이 늦게 수행된 행동보다 보상에 더 큰 영향을 준다. (최종 행동은 최종 보상에만 영향을 주지만, 첫 행동은 모든 보상에 영향을 미친다.)
  - 즉, 처음 행동들은 그래디언트가 크고, 마지막 행동들은 그래디언트가 작다. (ill-conditioned problem)
  - 하지만 여기서는 LQR과 같은 2차 방법을 사용할 수 없다.  왜냐하면 정책 파라미터는 모든 시간스텝과 관련되어 있기 때문에 동적 프로그래밍을 사용할 수 없다.(?)
  - 딥러닝 관점에서는 RNN을 학습시키는 문제와 비슷하다. (vanishing and exploding gradients)
  - 하지만 RNN의 LSTM과 같은 방식을 사용할 수는 없다. (왜냐하면 다이나믹을 임의로 선택할 수 없기 때문이다.)
- p9: 해결책
  - derivative-free(model-free) RL을 사용하자.
  - 여기서 모델은 가상 샘플을 생성하는데 사용한다.
- p11: model-free optimization with model
  - policy gradient가 backprop gradient 보다 안정적이다. (backprop gradient는 많은 Jacobian 곱셈이 필요?)
- p12: model-based RL with policy gradient (ver 2.5)
  - 학습된 dynamics로부터 샘플을 생성하고 policy gradient를 적용한다.(데이터가 충분하면 policy gradient는 잘 작동한다)
  - 하지만 학습된 모델과 true dynamics의 차이 때문에 오차가 누적되는 문제가 발생한다. 시간이 길어질수록 오차가 커진다. $O(\epsilon T^2)$
  - short rollout으로 이 문제를 해결할 수 있다.
- p15: model-based RL with short rollouts
  - 데이터셋에서 샘플을 선택하고, 모델을 이용해 short rollout 수행한다.
  - real 데이터와 model 데이터를 모두 사용해 사용해 정책을 개선한다. (off-policy RL)
- p18: Dyna 알고리즘 (online Q-learning, model-free RL with model)
  - 환경과 상호작용을 통해 데이터 $(s,a,s',r$)를 얻는다. (리플레이 버퍼에 저장한다)
  - 이 데이터를 이용해 모델과 Q 함수를 업데이트한다.
  - 리플레이 버퍼에서 데이터를 샘플링해서 Q 함수를 업데이트한다. (여러번 반복)
- p19: general dyna-style model-based RL
  - real data를 이용해 모델을 학습시킨다.
  - 리플레이 버퍼에서 상태 $s$를 샘플링하고, 행동 $a$을 선택한다(버퍼/정책/랜덤).
  - 모델을 이용해 다음 상태를 시뮬레이션한다. ($s',r$을 생성)
  - model-free RL로 데이터 $(s,a,s',r)$을 학습한다.
- p21: MBA, MVE, MBPO
  - 이 방법과 p19 Dyna-style의 차이점은? 리플레이 버퍼를 이용해 모델을 학습시킨다.
  - 이 방법의 장점과 단점은?
